# Evaluation Plan & Experimental Results

This document outlines the evaluation strategy for the "Automated Technology Landscaping" paper. The goal is to quantitatively and qualitatively demonstrate that the generated taxonomy is **structurally sound**, **semantically distinct**, and **distributionally balanced**.

## 1. Dimension: Distributional Balance (The "Fit")
*Does the taxonomy divide the landscape efficiently, or are there "mega-categories" that hide detail?*

### Metric 1.1: Normalized Taxonomy Entropy (NTE)
To measure how balanced the distribution of works is across the leaf nodes of the taxonomy.
*   **Formulation**:
    Let $C = \{c_1, ..., c_k\}$ be the set of leaf categories.
    Let $p_i$ be the proportion of total works assigned to category $c_i$.
    $$ H(C) = -\sum_{i=1}^{k} p_i \log_2(p_i) $$
    $$ NTE = \frac{H(C)}{\log_2(k)} $$
*   **Interpretation**: An NTE close to 1.0 indicates a perfectly uniform distribution (maximum information). An NTE close to 0 indicates that all works are clumped into a single category.
*   **Target**: We aim for a high NTE, acknowledging that real-world phenomena often follow a power law (Zipfian distribution), but we want to avoid trivial taxonomies.

### Metric 1.2: The Gini Coefficient of Granularity
*   **Formulation**: Standard Gini coefficient applied to the set of category sizes $\{|c_1|, ..., |c_k|\}$.
*   **Interpretation**: Measures inequality in category sizes. A lower Gini coefficient suggests the taxonomy has found a consistent level of abstraction (granularity) across different branches.

### Figure 1: The Lorenz Curve of Landscape Granularity
*   **Type**: Line Plot.
*   **X-Axis**: Cumulative percentage of categories (sorted by size).
*   **Y-Axis**: Cumulative percentage of works covered.
*   **Visual Proof**: A curve closer to the diagonal (y=x) indicates a balanced taxonomy. A sharp "elbow" indicates that a few broad categories dominate the landscape (bad).

---

## 2. Dimension: Semantic Distinctiveness (The "Non-Overlapping" Rule)
*Are the categories distinct, or do they bleed into each other?*

### Metric 2.1: Silhouette Score of Classifications
We treat the taxonomy generation as a clustering task. We use pre-trained embeddings (e.g., OpenAI `text-embedding-3-small`) of the work abstracts.
*   **Formulation**: Calculate the standard Silhouette Coefficient for the works based on their assigned taxonomy labels.
    $$ s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}} $$
    Where $a(i)$ is mean distance to own category, $b(i)$ is mean distance to nearest neighbor category.
*   **Interpretation**: Positive values indicate that works are semantically closer to their assigned category than to any other category.

### Metric 2.2: Inter-Category Semantic Overlap
*   **Formulation**: Pairwise Cosine Similarity between the embeddings of the *Category Descriptions* generated by the LLM.
*   **Interpretation**: We want to minimize the average pairwise similarity between sibling nodes. High similarity implies redundancy.

### Figure 2: The Semantic Separation Map (t-SNE/UMAP)
*   **Type**: Scatter Plot.
*   **Data**: 2D projection of the embeddings of all research works.
*   **Encoding**: Color points by their assigned Top-Level Category.
*   **Visual Proof**: We should see distinct "islands" of colors. If the colors are mixed like confetti, the taxonomy fails to capture the underlying semantic structure of the vector space.

---

## 3. Dimension: Comprehensiveness & Coherence (The "Quality")
*Does the taxonomy actually describe the content?*

### Metric 3.1: Semantic Centroid Distance
*   **Formulation**: The average cosine distance between a work's embedding and the embedding of its assigned category's *name/description*.
*   **Interpretation**: Lower distance means the category name accurately describes the underlying papers.

### Metric 3.2: LLM-as-a-Judge Coherence Score
*   **Method**: Sample 50 random works and their assigned categories. Feed them to a superior model (e.g., GPT-4o) with the prompt: *"Does this abstract belong in this category? Rate 1-5."*
*   **Interpretation**: Provides a human-proxy qualitative metric.

### Figure 3: The "Unclassified" Drop-off
*   **Type**: Bar Chart.
*   **Comparison**: Compare our method vs. a baseline (e.g., standard LDA Topic Modeling or K-Means).
*   **Y-Axis**: Semantic Centroid Distance (lower is better).
*   **Visual Proof**: Demonstrates that our hierarchical approach fits the data better than flat clustering.

---

## 4. Summary of Figures for the Paper

| Figure # | Title | Purpose | X-Axis | Y-Axis |
| :--- | :--- | :--- | :--- | :--- |
| **Fig 1** | **Taxonomy Visualization** | The "Hero" shot (from `draw_taxonomy.py`). Shows the full structure. | Depth | Layout Position |
| **Fig 2** | **Category Distribution (Lorenz Curve)** | Proves the taxonomy is balanced and not lopsided. | % of Categories | % of Works |
| **Fig 3** | **Semantic Projection (UMAP)** | Proves categories are non-overlapping and distinct. | UMAP-1 | UMAP-2 |
| **Fig 4** | **Coherence vs. Depth** | Shows that as we go deeper, specificity increases without losing coherence. | Taxonomy Depth | Semantic Similarity (Work-to-Category) |

